{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB1UYcAIG4Yf"
      },
      "source": [
        "Models Used :-\n",
        "\n",
        "We used the Mask RCNN model and trained it on our datset for 5 epochs.\n",
        "We used a Feedforward neural network and trained it for 7 epochs.\n",
        "\n",
        "Finetuning:-\n",
        "\n",
        "For the Finetuning of classification model we used droputs,callbacks, normalization, autotune of the training and validation dataset\n",
        "\n",
        "Pipelines:-\n",
        "\n",
        "1.Mask RCNN-for health monitor segmentation\n",
        "2.Classification model to classify the type of monitor\n",
        "3.We used Tesseract OCR for digit extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7wslkuF758Z"
      },
      "source": [
        "**Installing and unzipping the neccesary repositories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCEsHQBq7gOe",
        "outputId": "86524751-9548-484a-ceb7-af7a2377dfb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-02-07 17:32:15--  https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 430460776 (411M) [application/x-deb]\n",
            "Saving to: ‘libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb’\n",
            "\n",
            "libcudnn8_8.1.0.77- 100%[===================>] 410.52M   200MB/s    in 2.0s    \n",
            "\n",
            "2023-02-07 17:32:17 (200 MB/s) - ‘libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb’ saved [430460776/430460776]\n",
            "\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.1.1.33-1+cuda11.2 to 8.1.0.77-1+cuda11.2\n",
            "(Reading database ... 129496 files and directories currently installed.)\n",
            "Preparing to unpack libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.1.1.33-1+cuda11.2) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n",
            "lrwxrwxrwx 1 root root     17 Jan 25  2021 /usr/lib/x86_64-linux-gnu/libcudnn.so.8 -> libcudnn.so.8.1.0\n",
            "-rw-r--r-- 1 root root 158264 Jan 25  2021 /usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.5.0\n",
            "  Downloading tensorflow-2.5.0-cp38-cp38-manylinux2010_x86_64.whl (454.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.4/454.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
            "Collecting grpcio~=1.34.0\n",
            "  Downloading grpcio-1.34.1-cp38-cp38-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (2.9.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (1.12)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.6.0,>=2.5.0rc0\n",
            "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.4/462.4 KB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (0.38.4)\n",
            "Collecting termcolor~=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (3.19.6)\n",
            "Collecting keras-nightly~=2.5.0.dev\n",
            "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.16.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.2.2)\n",
            "Building wheels for collected packages: termcolor, wrapt\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=b49f140c3b831f0f834f20a10beb3f8f2f6f1b9c15b83d07d2ee61baddf74841\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/16/9c/5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-linux_x86_64.whl size=78558 sha256=460b1d81eba7ef6e454a7ad2b95f21107e19a9843307b27a11ceca2eee6d04e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/fd/9e/b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
            "Successfully built termcolor wrapt\n",
            "Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, keras-nightly, numpy, grpcio, absl-py, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.4.0\n",
            "    Uninstalling typing_extensions-4.4.0:\n",
            "      Successfully uninstalled typing_extensions-4.4.0\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.2.0\n",
            "    Uninstalling termcolor-2.2.0:\n",
            "      Successfully uninstalled termcolor-2.2.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.51.1\n",
            "    Uninstalling grpcio-1.51.1:\n",
            "      Successfully uninstalled grpcio-1.51.1\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.4.0\n",
            "    Uninstalling absl-py-1.4.0:\n",
            "      Successfully uninstalled absl-py-1.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "pydantic 1.10.4 requires typing-extensions>=4.2.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
            "jaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "grpcio-status 1.48.2 requires grpcio>=1.48.2, but you have grpcio 1.34.1 which is incompatible.\n",
            "google-cloud-bigquery 3.4.2 requires grpcio<2.0dev,>=1.47.0, but you have grpcio 1.34.1 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed absl-py-0.15.0 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 numpy-1.19.5 tensorflow-2.5.0 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n",
            "--2023-02-07 17:34:44--  https://pysource.com/extra_files/maskrcnn_colab_demo_commit_17.zip\n",
            "Resolving pysource.com (pysource.com)... 172.67.180.33, 104.21.67.193, 2606:4700:3036::ac43:b421, ...\n",
            "Connecting to pysource.com (pysource.com)|172.67.180.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59340 (58K) [application/zip]\n",
            "Saving to: ‘maskrcnn_colab_demo_commit_17.zip’\n",
            "\n",
            "maskrcnn_colab_demo 100%[===================>]  57.95K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-02-07 17:34:45 (4.90 MB/s) - ‘maskrcnn_colab_demo_commit_17.zip’ saved [59340/59340]\n",
            "\n",
            "Archive:  maskrcnn_colab_demo_commit_17.zip\n",
            "   creating: maskrcnn_colab/\n",
            "   creating: maskrcnn_colab/mrcnn_demo/\n",
            "  inflating: maskrcnn_colab/mrcnn_demo/config.py  \n",
            "  inflating: maskrcnn_colab/mrcnn_demo/model.py  \n",
            "  inflating: maskrcnn_colab/mrcnn_demo/m_rcnn.py  \n",
            "  inflating: maskrcnn_colab/mrcnn_demo/parallel_model.py  \n",
            "  inflating: maskrcnn_colab/mrcnn_demo/utils.py  \n",
            "  inflating: maskrcnn_colab/mrcnn_demo/visualize.py  \n",
            "VERS 0.4 - updated 04/08/2022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/maskrcnn_colab/mrcnn_demo/model.py:2378: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if os.name is 'nt':\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading pretrained model to /content/maskrcnn_colab/mask_rcnn_coco.h5 ...\n",
            "... done downloading pretrained model!\n"
          ]
        }
      ],
      "source": [
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb\n",
        "!dpkg -i libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb\n",
        "# Check if package has been installed\n",
        "!ls -l /usr/lib/x86_64-linux-gnu/libcudnn.so.*\n",
        "# Upgrade Tensorflow\n",
        "!pip install --upgrade tensorflow==2.5.0\n",
        "!wget https://pysource.com/extra_files/maskrcnn_colab_demo_commit_17.zip\n",
        "!unzip maskrcnn_colab_demo_commit_17.zip\n",
        "import sys\n",
        "sys.path.append(\"/content/maskrcnn_colab/mrcnn_demo\")\n",
        "from m_rcnn import *\n",
        "from visualize import random_colors, get_mask_contours, draw_mask\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjaKmGSLCmXq"
      },
      "source": [
        "**Mounting the Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhDEHUXI6FQc",
        "outputId": "e9f19a4d-9114-457a-8f15-af520b68b137"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "#uncomment if mounting the drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqJYIXBuC3ZM"
      },
      "source": [
        "**Models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z62FUgFN6a8f"
      },
      "outputs": [],
      "source": [
        "model1=\"\" ##mention the mnodel path explicitly\n",
        "model2=\"\"##mention the mnodel path explicitly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cykFupWIDJve"
      },
      "source": [
        "**Inference Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y9pzjtoawdV"
      },
      "outputs": [],
      "source": [
        "def inference(image_path):\n",
        "  '''\n",
        "  Function responsible for inference.\n",
        "  Args: \n",
        "    image_path: str, path to image file. eg. \"input/aveksha_micu_mon--209_2023_1_17_12_0_34.jpeg\"\n",
        "  Returns:\n",
        "    result: dict, final output dictionary. eg. {\"HR\":\"80\", \"SPO2\":\"98\", \"RR\":\"15\", \"SBP\":\"126\", \"DBP\":\"86\"}\n",
        "  '''\n",
        "  result = {}\n",
        "\n",
        "  ### put your code here\n",
        "  import os\n",
        "  from os import listdir\n",
        "  import cv2\n",
        "  import pandas as pd\n",
        "  import pytesseract\n",
        "  import csv\n",
        "  import json\n",
        "  import os\n",
        "  import numpy as np\n",
        "  from google.colab.patches import cv2_imshow\n",
        "\n",
        "  # check if the image ends with png or jpg or jpeg\n",
        "  img_n=cv2.imread(image_path)\n",
        "  r = model1.detect([img_n])[0]\n",
        "  colors = random_colors(1)\n",
        "  object_count = len(r[\"class_ids\"])\n",
        "  mask = r[\"masks\"][:, :, 0]\n",
        "  colors = random_colors(1)\n",
        "  contours = get_mask_contours(mask)\n",
        "  bbx=cv2.boundingRect(contours[0])\n",
        "  img_cropped=img_n[bbx[1]:bbx[1]+bbx[3],bbx[0]:bbx[0]+bbx[2]]\n",
        "  cv2_imshow(img_cropped)\n",
        "  test_img =img_cropped\n",
        "  img = np.expand_dims(test_img, axis = 0)\n",
        "  prediction=model2.predict(img)\n",
        "  score = tf.nn.softmax(prediction)\n",
        "  print(\"This image most likely belongs to {} with a {:.2f} percent confidence.\".format(class_names[np.argmax(score)], 100 * np.max(score)))\n",
        "\n",
        "\n",
        "  def extract_digits(cropped):\n",
        "\n",
        "    #pre-process\n",
        "    cropped = cv2.resize(cropped, (620, 350))\n",
        "    \n",
        "    gray = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
        "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n",
        "    \n",
        "    # cv2.imshow('processed', thresh)\n",
        "    # cv2.waitKey(0)\n",
        "    \n",
        "    \n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))\n",
        "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    kernel = np.ones((3,3),np.uint8)\n",
        "    threshold = cv2.morphologyEx(opening, cv2.MORPH_OPEN, kernel)\n",
        "    \n",
        "    \n",
        "    # Run OCR on the image\n",
        "    text = pytesseract.image_to_string(threshold, config='--psm 6')\n",
        "    # print(text)\n",
        "    \n",
        "    # Extract the digits from the OCR output\n",
        "    digits = [char for char in text if char.isdigit()]\n",
        "    \n",
        "    # Return the extracted digits as a string\n",
        "    return \"\".join(digits), text\n",
        "        \n",
        "        \n",
        "\n",
        "        \n",
        "  def use_ocr(bbox):\n",
        "    output = {}\n",
        "    xmin, ymin, xmax, ymax= bbox\n",
        "\n",
        "    bbox= [xmin, ymin, xmax, ymax]\n",
        "    #print(bbox)\n",
        "    cropped = img[int(ymin):int(ymax), int(xmin):int(xmax)]\n",
        "\n",
        "    #print(\"Extracted digits:\", \"\".join(digits))\n",
        "    #text= pytesseract.image_to_string(cropped, config='--psm 7 -c tessedit_char_whitelist=0123456789.%')\n",
        "\n",
        "    # Extract the digits from the cropped image\n",
        "    digits, text = extract_digits(cropped)\n",
        "    # print(column_names[i], digits)\n",
        "\n",
        "    # Store the extracted digits in the results dictionary\n",
        "    return digits, text\n",
        "\n",
        "    #         post_process(output)\n",
        "    \n",
        "  classif=class_names[np.argmax(score)]\n",
        "  output={}\n",
        "  if classif==\"BPL-EliteView-EV10-B_Meditec-England-A\":\n",
        "    hr= [0,0,670,323] #left upper\n",
        "    spo2= [0,329,663,391+663] # left lower\n",
        "    rr=[675,0,605+675,328]# right upper\n",
        "    clubbed =[670,335,610+670,385+335] #right lower\n",
        "    v1,t= use_ocr(hr); output[\"HR\"]=v1\n",
        "    v2,t= use_ocr(spo2); output[\"SPO2\"]=v2\n",
        "    v3,t3= use_ocr(rr)\n",
        "    v4,t4= use_ocr(clubbed)\n",
        "    if(len(t3)>=4):\n",
        "        getValues_1(output, v3, t3)\n",
        "        output[\"RR\"]=v4\n",
        "    else:\n",
        "        getValues_1(output, v4, t4)\n",
        "        output[\"RR\"]=v3\n",
        "    \n",
        "  elif classif==\"BPL-EliteView-EV100-C\":\n",
        "    hr_bp=[865,38,415+865,205+38]\n",
        "    spo2_rr=[859,297,370+859,254+297]\n",
        "    v1,t= use_ocr(hr_bp)\n",
        "    v2,t1= use_ocr(spo2_rr)\n",
        "    print(v1, v2)\n",
        "    output[\"HR\"]=int(v1[0:2])\n",
        "    output[\"MAP\"]=int(v1)%100\n",
        "    \n",
        "  elif classif==\"BPL-Ultima-PrimeD-A\":\n",
        "    hr=[815,0,465+815,258]\n",
        "    spo2=[815,272,465+815,226+272]\n",
        "    rr=[815,512,465+815,207+512]\n",
        "    bp=[0,513,806,207+513]\n",
        "    v1,t= use_ocr(hr)\n",
        "    v2,t= use_ocr(spo2)\n",
        "    v3,t= use_ocr(rr)\n",
        "    v4,t= use_ocr(bp)\n",
        "  #     print(v1, v2)\n",
        "    output[\"HR\"]=v1\n",
        "    output[\"SPO2\"]=v2\n",
        "    output[\"RR\"]=v3\n",
        "    output[\"MAP\"]=int(v4)%100\n",
        "    output[\"DBP\"]=int(v4/100)%100\n",
        "\n",
        "    \n",
        "  elif classif==\"Nihon-Kohden-lifescope-A\":\n",
        "    hr=[0,39,476,232+39]\n",
        "    spo2=[0,465,473,139+465]\n",
        "    rr=[0,610,471,104+610]\n",
        "    bp=[0,278,477,181+278]\n",
        "    v1,t= use_ocr(hr)\n",
        "    v2,t= use_ocr(spo2)\n",
        "    v3,t= use_ocr(rr)\n",
        "    v4,t= use_ocr(bp)\n",
        "  #     print(v1, v2)\n",
        "    output[\"HR\"]=v1\n",
        "    output[\"SPO2\"]=v2\n",
        "    output[\"RR\"]=v3\n",
        "    output[\"MAP\"]=int(v4)%100\n",
        "    output[\"DBP\"]=(int(v4)/100)%100\n",
        "\n",
        "\n",
        "  result=output\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmXbgqNGDP_2"
      },
      "source": [
        "**Calling the Inference Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyCM6oNz6k8I"
      },
      "outputs": [],
      "source": [
        "inference(\"\")##Provide the image path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEeEPhC168FY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
